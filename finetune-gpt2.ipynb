{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 19:24:00.523653: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-11 19:24:01.079983: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 19:24:04.923990: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jinran/anaconda3/envs/LLM_task/lib/python3.11/site-packages/transformers/utils/generic.py:482: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, accuracy_score, f1_score\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 4\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "GPU 1: Tesla V100-SXM2-32GB\n",
      "GPU 2: Tesla V100-SXM2-32GB\n",
      "GPU 3: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)  # Now this will refer to the third GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print cuda name\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    # Iterate over the available GPUs and print their names\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: 0\n",
      "Device name: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = df\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"cleaned_text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"target\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "    \n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    outputs = model(input_batch)\n",
    "    logits = outputs.logits[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n",
    "\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_batch)\n",
    "                logits = outputs.logits[:, -1, :]  # Logits of last output token\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    " \n",
    "\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    " \n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    " \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1             Forest fire near La Ronge Sask. Canada       1\n",
      "2  All residents asked to 'shelter in place' are ...       1\n",
      "3  13,000 people receive #wildfires evacuation or...       1\n",
      "4  Just got sent this photo from Ruby #Alaska as ...       1\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv', usecols = ['text','target'])\n",
    "\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s\\.\\,\\!\\?\\']', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_text'] = train['text'].apply(clean_text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>, people receive wildfires evacuation orders i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "      <td>rockyfire update california hwy. closed in bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "      <td>flood disaster heavy rain causes flash floodin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "5  #RockyFire Update => California Hwy. 20 closed...       1   \n",
       "6  #flood #disaster Heavy rain causes flash flood...       1   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  our deeds are the reason of this earthquake ma...  \n",
       "1             forest fire near la ronge sask. canada  \n",
       "2  all residents asked to 'shelter in place' are ...  \n",
       "3  , people receive wildfires evacuation orders i...  \n",
       "4  just got sent this photo from ruby alaska as s...  \n",
       "5  rockyfire update california hwy. closed in bot...  \n",
       "6  flood disaster heavy rain causes flash floodin...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your DataFrame is named 'df' and contains a column 'score'\n",
    "train_df, val_df = train_test_split(train, test_size=0.2, stratify=train['target'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "window_size = 55\n",
    "\n",
    "train_dataset = RatingDataset(\n",
    "    df=train_df,\n",
    "    max_length=window_size,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "val_dataset = RatingDataset(\n",
    "    df=val_df,\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer)\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Overall Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinran/anaconda3/envs/LLM_task/lib/python3.11/site-packages/transformers/utils/generic.py:339: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jinran/anaconda3/envs/LLM_task/lib/python3.11/site-packages/transformers/utils/generic.py:339: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "model.lm_head = torch.nn.Linear(in_features=768, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 124,441,346\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "# Overall the same as `train_model_simple` in chapter 5\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    #model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous epoch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "        print('finished 1 epoch')\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n",
    "        \n",
    "\n",
    "# Same as chapter 5\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.module.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_confusion_matrix(model, device, data_loader):\n",
    "    # Assuming you have your trained model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Initialize empty lists to store true labels and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # Iterate over the test data loader\n",
    "    \n",
    "    for text, labels in data_loader:\n",
    "        # Move the data to the same device as the model (GPU or CPU)\n",
    "        text = text.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(text)\n",
    "            logits = outputs.logits[:, -1, :]  # Logits of last output token\n",
    "        predicted = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "        # Append true labels and predicted labels to the lists\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "        \n",
    "    # print accuracy\n",
    "    acc = accuracy_score(true_labels, predicted_labels)\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    print('F1')\n",
    "    print(f'f1: {f1:.4f}')\n",
    "    return f1, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Finding optimal threshold\"):\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, -1, :]  # Logits of last output token\n",
    "            probs = torch.softmax(logits, dim=-1)[:, 1]  # Probability of positive class\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    precisions, recalls, thresholds = precision_recall_curve(all_labels, all_probs)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "    optimal_idx = f1_scores[:-1].argmax()  # Exclude the last element as it's undefined\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold\n",
    "\n",
    "def create_submission_df(model, test_df, tokenizer, device, val_loader, batch_size=32):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    optimal_threshold = find_optimal_threshold(model, val_loader, device)\n",
    "    print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "    \n",
    "    # Create a DataLoader for the test data\n",
    "    test_dataset = RatingDataset(test_df, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting targets\"):\n",
    "            input_ids, _ = batch  # Unpack the batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, -1, :]  # Logits of last output token\n",
    "            probs = torch.softmax(logits, dim=-1)[:, 1]  # Probability of positive class\n",
    "            predicted = (probs > optimal_threshold).int()\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # Create the submission dataframe\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'target': predictions\n",
    "    })\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO：\n",
    "optimize submission batch size, perhaps no difference at all \\\n",
    "try stacking models \\\n",
    "try different number of epochs \\\n",
    "try different LR \\\n",
    "try different weight decay \\\n",
    "Maybe make a grid search or cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe03c18cf10>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cu118'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning rates:   0%|                                                                             | 0/1 [00:00<?, ?it/s]\n",
      "Batch sizes:   0%|                                                                                | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Weight decays:   0%|                                                                              | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "Accuracy: 0.8418\n",
      "Confusion Matrix:\n",
      "[[788  81]\n",
      " [160 494]]\n",
      "F1\n",
      "f1: 0.8039\n",
      "LR: 1e-05, Batch Size: 16, Weight Decay: 0.01, Epochs: 4, Accuracy: 0.8418, F1: 0.8039\n",
      "finished run 1 ============================\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "Accuracy: 0.7840\n",
      "Confusion Matrix:\n",
      "[[653 216]\n",
      " [113 541]]\n",
      "F1\n",
      "f1: 0.7668\n",
      "LR: 1e-05, Batch Size: 16, Weight Decay: 0.01, Epochs: 6, Accuracy: 0.7840, F1: 0.7668\n",
      "finished run 2 ============================\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Weight decays:  20%|█████████████▊                                                       | 1/5 [08:05<32:21, 485.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8050\n",
      "Confusion Matrix:\n",
      "[[703 166]\n",
      " [131 523]]\n",
      "F1\n",
      "f1: 0.7789\n",
      "LR: 1e-05, Batch Size: 16, Weight Decay: 0.01, Epochs: 8, Accuracy: 0.8050, F1: 0.7789\n",
      "finished run 3 ============================\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "Accuracy: 0.8181\n",
      "Confusion Matrix:\n",
      "[[709 160]\n",
      " [117 537]]\n",
      "F1\n",
      "f1: 0.7950\n",
      "LR: 1e-05, Batch Size: 16, Weight Decay: 0.05, Epochs: 4, Accuracy: 0.8181, F1: 0.7950\n",
      "finished run 4 ============================\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "Accuracy: 0.8070\n",
      "Confusion Matrix:\n",
      "[[691 178]\n",
      " [116 538]]\n",
      "F1\n",
      "f1: 0.7854\n",
      "LR: 1e-05, Batch Size: 16, Weight Decay: 0.05, Epochs: 6, Accuracy: 0.8070, F1: 0.7854\n",
      "finished run 5 ============================\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Weight decays:  40%|███████████████████████████▌                                         | 2/5 [16:09<24:13, 484.63s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8050\n",
      "Confusion Matrix:\n",
      "[[690 179]\n",
      " [118 536]]\n",
      "F1\n",
      "f1: 0.7831\n",
      "LR: 1e-05, Batch Size: 16, Weight Decay: 0.05, Epochs: 8, Accuracy: 0.8050, F1: 0.7831\n",
      "finished run 6 ============================\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "Accuracy: 0.7787\n",
      "Confusion Matrix:\n",
      "[[620 249]\n",
      " [ 88 566]]\n",
      "F1\n",
      "f1: 0.7706\n",
      "LR: 1e-05, Batch Size: 16, Weight Decay: 0.1, Epochs: 4, Accuracy: 0.7787, F1: 0.7706\n",
      "finished run 7 ============================\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n",
      "Accuracy: 0.8155\n",
      "Confusion Matrix:\n",
      "[[712 157]\n",
      " [124 530]]\n",
      "F1\n",
      "f1: 0.7905\n",
      "LR: 1e-05, Batch Size: 16, Weight Decay: 0.1, Epochs: 6, Accuracy: 0.8155, F1: 0.7905\n",
      "finished run 8 ============================\n",
      "finished 1 epoch\n",
      "finished 1 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Weight decays:  60%|█████████████████████████████████████████▍                           | 3/5 [24:12<16:08, 484.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8293\n",
      "Confusion Matrix:\n",
      "[[756 113]\n",
      " [147 507]]\n",
      "F1\n",
      "f1: 0.7959\n",
      "LR: 1e-05, Batch Size: 16, Weight Decay: 0.1, Epochs: 8, Accuracy: 0.8293, F1: 0.7959\n",
      "finished run 9 ============================\n",
      "finished 1 epoch\n"
     ]
    }
   ],
   "source": [
    "num_workers = 4\n",
    "results = []\n",
    "#Grid\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-5],\n",
    "    'batch_size': [16, 32],\n",
    "    'weight_decay': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'epochs': [4, 6, 8]\n",
    "}\n",
    "count = 1\n",
    "for lr in tqdm(param_grid['learning_rate'], desc=\"Learning rates\"):\n",
    "    for batch_size in tqdm(param_grid['batch_size'], desc=\"Batch sizes\", leave=False):\n",
    "        train_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=num_workers,\n",
    "                drop_last=True,\n",
    "            )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "                dataset=val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                drop_last=False,\n",
    "            )\n",
    "        for weight_decay in tqdm(param_grid['weight_decay'], desc=\"Weight decays\", leave=False):\n",
    "            # Initialize model and optimizer\n",
    "            model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "            model.lm_head = torch.nn.Linear(in_features=768, out_features=2)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "            for epoch in range(1, max(param_grid['epochs']) + 1):\n",
    "                # Train for one epoch\n",
    "                train_classifier_simple(model, train_loader, val_loader, optimizer, device, \n",
    "                                            num_epochs=1, eval_freq=len(train_loader), eval_iter=None, \n",
    "                                            tokenizer=None)  # Assuming tokenizer is not needed here\n",
    "                \n",
    "                # If this epoch is in param_grid['epochs'], print the results\n",
    "                if epoch in param_grid['epochs']:\n",
    "                    # Evaluate\n",
    "                    val_f1, val_accuracy = accuracy_confusion_matrix(model, device, val_loader)\n",
    "                    results.append({\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'weight_decay': weight_decay,\n",
    "                    'epochs': epoch,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'val_f1_score': val_f1\n",
    "                })\n",
    "                    print(f\"LR: {lr}, Batch Size: {batch_size}, Weight Decay: {weight_decay}, \"\n",
    "                          f\"Epochs: {epoch}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}\")\n",
    "                    print(f'finished run {count} ============================')\n",
    "                    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV\n",
    "df_results.to_csv('grid_search_results_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   0%|                                                                      | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 8\n",
      "weight_decay 0.01\n",
      "run 2 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 8\n",
      "weight_decay 0.05\n",
      "run 3 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 8\n",
      "weight_decay 0.1\n",
      "run 4 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 8\n",
      "weight_decay 0.15\n",
      "run 5 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 8\n",
      "weight_decay 0.2\n",
      "run 6 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 16\n",
      "weight_decay 0.01\n",
      "run 7 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 16\n",
      "weight_decay 0.05\n",
      "run 8 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 16\n",
      "weight_decay 0.1\n",
      "run 9 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 16\n",
      "weight_decay 0.15\n",
      "run 10 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 16\n",
      "weight_decay 0.2\n",
      "run 11 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 32\n",
      "weight_decay 0.01\n",
      "run 12 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 32\n",
      "weight_decay 0.05\n",
      "run 13 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 32\n",
      "weight_decay 0.1\n",
      "run 14 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 32\n",
      "weight_decay 0.15\n",
      "run 15 ==================\n",
      "lr 1e-05\n",
      "epoch 4\n",
      "batch_size 32\n",
      "weight_decay 0.2\n",
      "run 16 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 8\n",
      "weight_decay 0.01\n",
      "run 17 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 8\n",
      "weight_decay 0.05\n",
      "run 18 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 8\n",
      "weight_decay 0.1\n",
      "run 19 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 8\n",
      "weight_decay 0.15\n",
      "run 20 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 8\n",
      "weight_decay 0.2\n",
      "run 21 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 16\n",
      "weight_decay 0.01\n",
      "run 22 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 16\n",
      "weight_decay 0.05\n",
      "run 23 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 16\n",
      "weight_decay 0.1\n",
      "run 24 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 16\n",
      "weight_decay 0.15\n",
      "run 25 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 16\n",
      "weight_decay 0.2\n",
      "run 26 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 32\n",
      "weight_decay 0.01\n",
      "run 27 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 32\n",
      "weight_decay 0.05\n",
      "run 28 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 32\n",
      "weight_decay 0.1\n",
      "run 29 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 32\n",
      "weight_decay 0.15\n",
      "run 30 ==================\n",
      "lr 1e-05\n",
      "epoch 6\n",
      "batch_size 32\n",
      "weight_decay 0.2\n",
      "run 31 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 8\n",
      "weight_decay 0.01\n",
      "run 32 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 8\n",
      "weight_decay 0.05\n",
      "run 33 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 8\n",
      "weight_decay 0.1\n",
      "run 34 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 8\n",
      "weight_decay 0.15\n",
      "run 35 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 8\n",
      "weight_decay 0.2\n",
      "run 36 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 16\n",
      "weight_decay 0.01\n",
      "run 37 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 16\n",
      "weight_decay 0.05\n",
      "run 38 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 16\n",
      "weight_decay 0.1\n",
      "run 39 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 16\n",
      "weight_decay 0.15\n",
      "run 40 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 16\n",
      "weight_decay 0.2\n",
      "run 41 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 32\n",
      "weight_decay 0.01\n",
      "run 42 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 32\n",
      "weight_decay 0.05\n",
      "run 43 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 32\n",
      "weight_decay 0.1\n",
      "run 44 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 32\n",
      "weight_decay 0.15\n",
      "run 45 ==================\n",
      "lr 1e-05\n",
      "epoch 8\n",
      "batch_size 32\n",
      "weight_decay 0.2\n",
      "run 46 ==================\n",
      "lr 3e-05\n",
      "epoch 4\n",
      "batch_size 8\n",
      "weight_decay 0.01\n",
      "run 47 ==================\n",
      "lr 3e-05\n",
      "epoch 4\n",
      "batch_size 8\n",
      "weight_decay 0.05\n",
      "run 48 ==================\n",
      "lr 3e-05\n",
      "epoch 4\n",
      "batch_size 8\n",
      "weight_decay 0.1\n",
      "run 49 ==================\n",
      "lr 3e-05\n",
      "epoch 4\n",
      "batch_size 8\n",
      "weight_decay 0.15\n",
      "run 50 ==================\n",
      "lr 3e-05\n",
      "epoch 4\n",
      "batch_size 8\n",
      "weight_decay 0.2\n",
      "run 51 ==================\n",
      "lr 3e-05\n",
      "epoch 4\n",
      "batch_size 16\n",
      "weight_decay 0.01\n",
      "run 52 ==================\n",
      "lr 3e-05\n",
      "epoch 4\n",
      "batch_size 16\n",
      "weight_decay 0.05\n",
      "run 53 ==================\n",
      "lr 3e-05\n",
      "epoch 4\n",
      "batch_size 16\n",
      "weight_decay 0.1\n",
      "run 54 ==================\n",
      "lr 3e-05\n",
      "epoch 4\n",
      "batch_size 16\n",
      "weight_decay 0.15\n",
      "Ep 1 (Step 000000): Train loss 2.206, Val loss 1.899\n",
      "Ep 1 (Step 000025): Train loss 0.700, Val loss 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  59%|███████████████████████████████████▉                         | 53/90 [00:08<00:05,  6.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 52\u001b[0m\n\u001b[1;32m     44\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     45\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m     46\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     47\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m     48\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m train_losses, val_losses, train_accs, val_accs, examples_seen \u001b[38;5;241m=\u001b[39m train_classifier_simple(\n\u001b[1;32m     53\u001b[0m     model, train_loader, val_loader, optimizer, device,\n\u001b[1;32m     54\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mepochs, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, eval_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     55\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     59\u001b[0m val_f1, val_accuracy \u001b[38;5;241m=\u001b[39m accuracy_confusion_matrix(model, device, val_loader)\n",
      "Cell \u001b[0;32mIn[39], line 17\u001b[0m, in \u001b[0;36mtrain_classifier_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, tokenizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 17\u001b[0m     input_batch, target_batch \u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mto(device), target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Reset loss gradients from previous epoch\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m calc_loss_batch(input_batch, target_batch, model, device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "count = 0\n",
    "# Define the hyperparameter ranges\n",
    "learning_rates = [1e-5, 3e-5, 5e-5, 7e-5]\n",
    "epochs_list = [4, 6, 8]\n",
    "batch_sizes = [8, 16, 32]\n",
    "weight_decays = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "\n",
    "# Create all combinations\n",
    "combinations = list(itertools.product(learning_rates, epochs_list, batch_sizes, weight_decays))\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "num_classes = 2\n",
    "num_workers = 4\n",
    "# Perform grid search\n",
    "for lr, epochs, batch_size, weight_decay in tqdm(combinations, desc=\"Grid Search Progress\"):\n",
    "    count += 1\n",
    "    print('run', count, '==================')\n",
    "    print('lr', lr)\n",
    "    print('epoch', epochs)\n",
    "    print('batch_size', batch_size)\n",
    "    print('weight_decay', weight_decay)\n",
    "    \n",
    "    # we hvae ran the first 36 combos\n",
    "    if count <= 53:\n",
    "        continue\n",
    "    \n",
    "    # Initialize model, optimizer, and data loaders with current hyperparameters\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "    model.lm_head = torch.nn.Linear(in_features=768, out_features=num_classes)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=epochs, eval_freq=25, eval_iter=5,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_f1, val_accuracy = accuracy_confusion_matrix(model, device, val_loader)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Epochs': epochs,\n",
    "        'Batch Size': batch_size,\n",
    "        'Weight Decay': weight_decay,\n",
    "        'Val Accuracy': val_accuracy,\n",
    "        'Val F1 Score': val_f1\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV\n",
    "df_results.to_csv('grid_search_results.csv', index=False)\n",
    "\n",
    "print(\"Grid search completed. Results saved to 'grid_search_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
